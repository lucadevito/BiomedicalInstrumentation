{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3c97d7",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "# Tracking position with visual markers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f26e16",
   "metadata": {},
   "source": [
    "Position tracking can be achieved even with a single camera when an object is recorded whose size is known a priori. When possible, it is convenient to use visual markers. Visual markers facilitate the feature detection as they have sharp edges and high-contrast colors. An example of visual markers are the ArUCo (Augmented Reality University of Cordoba) markers (see figure below), which were developed for the pose estimation in augmented reality applications. The main benefit of these markers is that a single marker provides enough correspondences (its four corners) to obtain the marker pose with respect to the camera frame. Also, the inner binary codification makes them specially robust, allowing the possibility of applying error detection and correction techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0ec74",
   "metadata": {},
   "source": [
    "<img src=\"arucoMarkers.jpg\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201fb9b8",
   "metadata": {},
   "source": [
    "The developer who propose ArUCo markers also shared a library using OpenCV to estimate their pose. After that, such library has been integrated into OpenCV. Therefore, we will use the python OpenCV library to get the pose of ArUCo markers and thus track the pose of the objects where the markers are placed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff7d312",
   "metadata": {},
   "source": [
    "## OpenCV basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24405d69",
   "metadata": {},
   "source": [
    "As above mentioned, we will use the Python OpenCV library. Here, some basic OpenCV functions are detailed. The OpenCV library is imported by the following import command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f2250",
   "metadata": {},
   "source": [
    "`import cv2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d6d70",
   "metadata": {},
   "source": [
    "Here is an example of how open a video for processing its frames:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13ff80",
   "metadata": {},
   "source": [
    "`video = cv2.VideoCapture(<filename>)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1eb52",
   "metadata": {},
   "source": [
    "If the argument is 0, the function will acquire the video using the default computer camera. To know the number of frames in the video, you can read one of the property of the video object:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278bc0d0",
   "metadata": {},
   "source": [
    "`nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87aa701",
   "metadata": {},
   "source": [
    "To get a frame from the video, you can use the read method:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26fcfc2",
   "metadata": {},
   "source": [
    "`ret, frame = video.read()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a138fe9",
   "metadata": {},
   "source": [
    "Once the processing is complete, you can release the video object as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad52c73",
   "metadata": {},
   "source": [
    "`video.release()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe9ef5",
   "metadata": {},
   "source": [
    "You may want to save a modified version of your video. To do it, you can use the VideoWriter class. An object of this class should be instantiated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f4b4b",
   "metadata": {},
   "source": [
    "`writer = cv2.VideoWriter(<filename>,                       # please pay attention that only .avi format is supported\n",
    "                           cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                           <fps>,                           # frames per second\n",
    "                           <frame size>)                    # frame size as a couple`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d55188",
   "metadata": {},
   "source": [
    "To write a frame, the following funciton is used:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6978973",
   "metadata": {},
   "source": [
    "`writer.write(frame)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea07fffd",
   "metadata": {},
   "source": [
    "Finally you should release the VideoWriter object:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6e8e0",
   "metadata": {},
   "source": [
    "`writer.release()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767fa387",
   "metadata": {},
   "source": [
    "## Calibrate the camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af51f630",
   "metadata": {},
   "source": [
    "The real position of an object in the image is related to the position of its image in the picture, by some parameters of the camera, such as the number of pixels, the sensor size and the focal length. Moreover, the lens often distort the image. For these reasons, the first step to be done is to calibrate our camera. The camera calibration allows us to estimate the transformation to apply to the image to get real world dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b8e46",
   "metadata": {},
   "source": [
    "Camera calibration is usually done using a chessboard image. OpenCV provides one, at the following link: [https://github.com/opencv/opencv/blob/master/doc/pattern.png](https://github.com/opencv/opencv/blob/master/doc/pattern.png) (I already downloaded and printed for you)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f387d",
   "metadata": {},
   "source": [
    "A set of images of the chessboard, at different distances and orientations should be acquired. You can store them in a specific folder. About 20 images are enough to get the calibration. Alternatively, you can take a video moving the chessboard and using the video frames to calibrate. Here is a sample code to calibrate the camera using the frames sampled from a video. After the calibration, the code stores the calibration parameters in a .yaml file for using them during measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb05f0",
   "metadata": {},
   "source": [
    "Take a video with the chessboard at different positions and orientations. Then, copy the video in a CameraCalibration folder inside the folder of this notebook. Finally, complete the code below with the name of the video and perform the calibration running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9e042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # Import the OpenCV library to enable computer vision\n",
    "import numpy as np # Import the NumPy scientific computing library\n",
    "import glob # Used to get retrieve files that have a specified pattern\n",
    "  \n",
    "# Chessboard dimensions\n",
    "number_of_squares_X = 10 # Number of chessboard squares along the x-axis\n",
    "number_of_squares_Y = 7  # Number of chessboard squares along the y-axis\n",
    "nX = number_of_squares_X - 1 # Number of interior corners along x-axis\n",
    "nY = number_of_squares_Y - 1 # Number of interior corners along y-axis\n",
    "square_size = 0.024 # Size, in meters, of a square side \n",
    "  \n",
    "# Set termination criteria. We stop either when an accuracy is reached or when\n",
    "# we have finished a certain number of iterations.\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001) \n",
    " \n",
    "# Define real world coordinates for points in the 3D coordinate frame\n",
    "# Object points are (0,0,0), (1,0,0), (2,0,0) ...., (5,8,0)\n",
    "object_points_3D = np.zeros((nX * nY, 3), np.float32)  \n",
    "  \n",
    "# These are the x and y coordinates                                              \n",
    "object_points_3D[:,:2] = np.mgrid[0:nY, 0:nX].T.reshape(-1, 2) \n",
    " \n",
    "object_points_3D = object_points_3D * square_size\n",
    " \n",
    "# Store vectors of 3D points for all chessboard images (world coordinate frame)\n",
    "object_points = []\n",
    "  \n",
    "# Store vectors of 2D points for all chessboard images (camera coordinate frame)\n",
    "image_points = []\n",
    "      \n",
    "## COMPLETE THE FOLLOWING LINE WITH THE FILE NAME OF THE CALIBRATION VIDEO\n",
    "cap = cv2.VideoCapture(\"????????????????????????\")\n",
    "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Sample one over 20 frames of the video\n",
    "for n in range(0,length,20):\n",
    "\n",
    "    # Load the image\n",
    "    print(f\"Reading frame %d of %d\" % (n//20+1,length//20+1))  \n",
    "    # Capture frame-by-frame\n",
    "    # This method returns True/False as well\n",
    "    # as the video frame.\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  \n",
    "\n",
    "    # Find the corners on the chessboard\n",
    "    success, corners = cv2.findChessboardCorners(gray, (nY, nX), None)\n",
    "\n",
    "    # If the corners are found by the algorithm, draw them\n",
    "    if success == True:\n",
    "\n",
    "      # Append object points\n",
    "      object_points.append(object_points_3D)\n",
    "\n",
    "      # Find more exact corner pixels       \n",
    "      corners_2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)       \n",
    "\n",
    "      # Append image points\n",
    "      image_points.append(corners_2)\n",
    "\n",
    "      # Draw the corners\n",
    "      cv2.drawChessboardCorners(frame, (nY, nX), corners_2, success)\n",
    "\n",
    "\n",
    "      # Perform camera calibration to return the camera matrix, distortion coefficients, rotation and translation vectors etc \n",
    "      ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(object_points, \n",
    "                                                        image_points, \n",
    "                                                        gray.shape[::-1], \n",
    "                                                        None, \n",
    "                                                        None)\n",
    "\n",
    "\n",
    "# Save parameters to a file\n",
    "print(\"Saving camera calibration parameters...\")\n",
    "cv_file = cv2.FileStorage('calibration_chessboard.yaml', cv2.FILE_STORAGE_WRITE)\n",
    "cv_file.write('K', mtx)\n",
    "cv_file.write('D', dist)\n",
    "cv_file.release()\n",
    "print(\"...done.\")\n",
    "\n",
    "# Load the parameters from the saved file\n",
    "cv_file = cv2.FileStorage('calibration_chessboard.yaml', cv2.FILE_STORAGE_READ) \n",
    "mtx = cv_file.getNode('K').mat()\n",
    "dst = cv_file.getNode('D').mat()\n",
    "cv_file.release()\n",
    "\n",
    "# Display key parameter outputs of the camera calibration process\n",
    "print(\"Camera matrix:\") \n",
    "print(mtx) \n",
    "\n",
    "print(\"\\n Distortion coefficient:\") \n",
    "print(dist) \n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f515c171",
   "metadata": {},
   "source": [
    "## ArUCo marker detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2402a13d",
   "metadata": {},
   "source": [
    "ArUCo markers are grouped into dictionaries depending on the size and properties of the markers. We will use the dictionary called DICT_4X4_1000, where markers are of size 4cm x 4cm. To detect an ArUCo marker the following steps are needed:\n",
    "\n",
    "1. recall the camera calibration parameters:\n",
    "\n",
    "        # Load the camera parameters from the saved file\n",
    "        cv_file = cv2.FileStorage(<camera_calibration_parameters_filename>, \n",
    "                                  cv2.FILE_STORAGE_READ) \n",
    "                                  mtx = cv_file.getNode('K').mat()\n",
    "                                  dst = cv_file.getNode('D').mat()\n",
    "                                  cv_file.release()\n",
    "\n",
    "2. get the used dictionary and create the detector object:\n",
    "\n",
    "        this_aruco_dictionary = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_4X4_1000)\n",
    "        this_aruco_parameters =  cv2.aruco.DetectorParameters()\n",
    "        this_aruco_detector = cv2.aruco.ArucoDetector(this_aruco_dictionary, this_aruco_parameters)\n",
    "     \n",
    "     \n",
    "3. Open the video, as described above.\n",
    "4. Read a new frame from the video and call the following function to detect the marker:\n",
    "\n",
    "        (corners, marker_ids, rejected) = this_aruco_detector.detectMarkers(frame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e900763e",
   "metadata": {},
   "source": [
    "The above function will return in marker_ids the list of the detected markers (we can have more than one marker in a single image).\n",
    "Please note that in the latest function, we need to pass the camera matrix and distortion coefficients obtained during the calibration. We may need to display the detected markers. In this case we can call:\n",
    "\n",
    "        cv2.aruco.drawDetectedMarkers(frame, corners, marker_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a17dc",
   "metadata": {},
   "source": [
    "## Pose estimation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e6f782e",
   "metadata": {},
   "source": [
    "To get the pose, we can call for each frame and for each marker the following function:\n",
    "\n",
    "        success, rvec, tvec = cv2.solvePnP(object_points, corners, mtx, dst, False, cv2.SOLVEPNP_IPPE_SQUARE)\n",
    "\n",
    "where:\n",
    "\n",
    "- `object_points`, is a matrix containing the real world positions of the marker corners, as shown below where $w$ is the sidelength of the marker:\n",
    "\n",
    "$$\\begin{bmatrix} -w/2 & w/2 & 0\\\\\n",
    "                 w/2 & w/2 & 0\\\\\n",
    "                 w/2 & -w/2 & 0\\\\\n",
    "                 -w/2 & w/2 & 0 \\end{bmatrix}$$\n",
    "  \n",
    "- `corners`, is the vector of the marker corners, returned by the `detectMarkers` function\n",
    "- `mtx`, is the camera calibration matrix\n",
    "- `dst`, is the distortion vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95884e4",
   "metadata": {},
   "source": [
    "The above function returns in tvecs the translation vectors, containing the real world position of the marker in the camera frame, and in rvecs the rotation vectors, which instead allows to get the marker orientation with respect to the camera frame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d8f6990",
   "metadata": {},
   "source": [
    "The rotation vector should first be converted to a rotation matrix. This is done by the following code:\n",
    "\n",
    "        rotation_matrix, jacob = cv2.Rodrigues(rvec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ffd4d09",
   "metadata": {},
   "source": [
    "To obtain the Euler angles, we can use the following code:\n",
    "\n",
    "        pose_mat = cv2.hconcat((rotation_matrix, tvec))\n",
    "        _, _, _, _, _, _, euler_angles = cv2.decomposeProjectionMatrix(pose_mat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36b00508",
   "metadata": {},
   "source": [
    "You may want to display the orientation of the marker in each frame by drawing the axes of the marker frame. The following code does this:\n",
    "\n",
    "        # Draw the axes on the marker\n",
    "        cv2.drawFrameAxes(frame, mtx, dst, rvec, tvec, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da88db",
   "metadata": {},
   "source": [
    "## Now it's your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027dcb22",
   "metadata": {},
   "source": [
    "Acquire a video where simple rotations are performed using a marker and complete the following Python script to draw the graph of the marker position and that of the Euler angles of the marker. The code should also produce a new video where the marker is marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2 # Import the OpenCV library to enable computer vision\n",
    "import numpy as np # Import the NumPy scientific computing library\n",
    "import math # Math library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Recall the camera calibration parameters\n",
    "## ADD HERE THE CORRESPONDING LINES (SEE TEXT ABOVE)\n",
    "\n",
    "# Get the used dictionary and create the detector object\n",
    "## ADD HERE THE CORRESPONDING LINES (SEE TEXT ABOVE)\n",
    "\n",
    "# Open the video and get the number of frames\n",
    "## ADD HERE THE CORRESPONDING LINES (SEE CALIBRATION CODE)\n",
    "\n",
    "# Open the VideoWriter object\n",
    "writer = cv2.VideoWriter(video_file[0:-4] + '_marked.avi', \n",
    "                           cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                           25, frame_size)\n",
    "\n",
    "# allocate a vector for each position and each Euler angle\n",
    "## ADD HERE THE CORRESPONDING LINES\n",
    "\n",
    "for n in range(nframes):\n",
    "    print(f\"frame {n+1} of {nframes}\") \n",
    "    \n",
    "    # Read a frame\n",
    "    ## ADD HERE THE CORRESPONDING LINES (SEE CALIBRATION CODE)\n",
    "    \n",
    "    # detect the markers\n",
    "    ## ADD HERE THE CORRESPONDING LINES (SEE TEXT ABOVE)\n",
    "    \n",
    "    # write detected marker onto the current frame\n",
    "    ## ADD HERE THE CORRESPONDING LINES (SEE TEXT ABOVE)\n",
    "    \n",
    "    # estimate marker pose\n",
    "    ## ADD HERE THE CORRESPONDING LINES (SEE TEXT ABOVE)\n",
    "    \n",
    "    # store marker position (x,y,z) into the corresponding vectors\n",
    "    ## ADD HERE THE CORRESPONDING LINES\n",
    "    \n",
    "    # write marker axes onto the current frame\n",
    "    ## ADD HERE THE CORRESPONDING LINES (SEE TEXT ABOVE)\n",
    "    \n",
    "    # convert rotation vector to rotation matrix, then to quaternion, and finally to Euler angles\n",
    "    ## ADD HERE THE CORRESPONDING LINES (SEE TEXT ABOVE)\n",
    "    \n",
    "    # store Euler angles in corresponding vectors\n",
    "    ## ADD HERE THE CORRESPONDING LINES (SEE TEXT ABOVE)\n",
    "    \n",
    "    # write the modified frame to the VideoWriter\n",
    "    writer.write(frame)\n",
    "\n",
    "# release video\n",
    "video.release()\n",
    "\n",
    "# release VideoWriter\n",
    "writer.release()\n",
    "\n",
    "# plot marker position graph\n",
    "## ADD HERE THE CORRESPONDING LINES\n",
    "\n",
    "# plot Euler angle graphs\n",
    "## ADD HERE THE CORRESPONDING LINES"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
